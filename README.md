<div align="center">

# ğŸ§  D-MMCE

### Dynamic Multi-Model Consensus Engine

**Aggregate "weak" LLM learners into a single Globally Optimal answer through ensemble intelligence.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-3776ab?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![asyncio](https://img.shields.io/badge/asyncio-parallel-yellow?style=for-the-badge)](https://docs.python.org/3/library/asyncio.html)
[![License: MIT](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)

<br>

<img src="https://img.shields.io/badge/OpenAI-GPT--4o-412991?style=for-the-badge&logo=openai&logoColor=white" alt="OpenAI">
<img src="https://img.shields.io/badge/Anthropic-Claude_3.5-D4A574?style=for-the-badge" alt="Anthropic">
<img src="https://img.shields.io/badge/Google-Gemini_1.5-4285F4?style=for-the-badge&logo=google&logoColor=white" alt="Google">
<img src="https://img.shields.io/badge/Ollama-Local_LLMs-000000?style=for-the-badge" alt="Ollama">

---

*One question Â· Multiple models Â· Multiple prompt framings Â· Peer review Â· Semantic clustering Â· Stability-verified synthesis*

**â†’ Works with cloud APIs, local LLMs via Ollama, or any combination.**

</div>

<br>

## ğŸ¯ The Problem

A single LLM can hallucinate, overfit to its training bias, or land on a **Local Optimum** â€” an answer that looks correct in isolation but fails under scrutiny.

D-MMCE treats every individual model output as a "weak learner" and applies an ensemble pipeline inspired by boosting, adversarial validation, and immune-system cross-reactivity to converge on a **Global Optimum**.

> **No API keys?** No problem. D-MMCE works entirely with local Ollama models â€” just `ollama pull` any models and go.

---

## âš¡ Key Features

| Feature | Description |
|---|---|
| ğŸ”€ **Diversity Injection** | Automatically generates 4 semantic prompt variants (original, step-by-step, adversarial, summary) to reduce framing bias |
| ğŸš€ **Parallel Inference** | `asyncio.as_completed()` fans out all model Ã— variant calls concurrently with live streaming as each completes |
| ğŸ“¡ **Token Streaming** | True SSE streaming from Ollama â€” tokens appear live in the UI as they're generated, eliminating timeout issues |
| ğŸ›¡ï¸ **Peer Review (Immune System)** | Cross-examination layer where models critique each other's outputs, generating a Contradiction Matrix |
| ğŸ¯ **Semantic Clustering** | `sentence-transformers` embeddings + HDBSCAN identify the consensus cluster; outliers (Local Optima) are discarded |
| âš–ï¸ **Meta-Judge + Stability Loop** | The lowest-penalty model synthesises the final answer, then re-verifies for convergence |
| âš¡ **Concurrency Control** | Configurable semaphore limits parallel tasks + exponential-backoff retry for transient failures |
| ğŸ¦™ **Local LLM Support** | Auto-discovers all Ollama models â€” use Llama, Qwen, Mistral, Phi, DeepSeek, or any other local model |
| ğŸŒ **Live Web UI** | Real-time pipeline visualization with streaming response cards, stability gauge, and full audit trail |
| ğŸ“œ **Run History** | SQLite-backed history of every pipeline run â€” browse, compare, and review past results |
| ğŸ“ **Debug Logging** | Every pipeline run is traced to `d_mmce.log` for easy debugging |

---

## ğŸ”¬ How It Works

```
 User Query
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prompt Perturbator    â”‚  â†’ 4 semantic variants (original, step-by-step,
â”‚   (Diversity Injection) â”‚    adversarial, summary)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Model Pool         â”‚  â†’ asyncio.as_completed() fans out P Ã— 4
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”     â”‚    concurrent calls. Responses stream to the
â”‚   â”‚GPT-4o â”‚Claude â”‚     â”‚    UI live as each model finishes.
â”‚   â”‚Gemini â”‚Ollama*â”‚     â”‚    * Any local model: Llama, Qwen, Mistralâ€¦
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Peer Reviewer       â”‚  â†’ "You are a logic auditorâ€¦"
â”‚    (Immune System)      â”‚    Pairwise cross-examination generates a
â”‚                         â”‚    Contradiction Matrix with penalty scores
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Semantic Clusterer    â”‚  â†’ sentence-transformers embeddings + HDBSCAN
â”‚   (Consensus Finder)    â”‚    Densest cluster = Consensus
â”‚                         â”‚    Outliers = Local Optima â†’ discarded
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Meta-Judge         â”‚  â†’ Lowest-penalty model synthesises the answer
â”‚   (Stability Loop)      â”‚    Re-runs until cosine_sim â‰¥ threshold
â”‚                         â”‚    Convergence â†’ Global Optimum âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     Final Verdict
   âœ… Globally Optimal Answer
```

### Local vs. Global Optimality

| Concept | In D-MMCE |
|---|---|
| **Local Optimum** | A response that seems plausible alone but diverges from ensemble consensus â€” falls outside the main semantic cluster or is flagged by peer reviewers |
| **Global Optimum** | The synthesised answer that (1) belongs to the densest semantic cluster, (2) survives peer review, and (3) remains **stable** across successive synthesis rounds |
| **Stability Loop** | If re-generating the synthesis yields a semantically different answer (cosine sim < threshold), the system treats it as a Local Optimum and retries. Convergence = Global Optimum |

---

## ğŸš€ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/your-username/D_MMCE.git
cd D_MMCE
python -m venv .venv

# Windows
.\.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate

pip install -r requirements.txt
```

### 2. Configure (pick any option)

<details>
<summary><b>Option A: Cloud APIs</b> â€” set API keys in <code>.env</code></summary>

```bash
cp .env.example .env
# Edit .env:
#   OPENAI_API_KEY=sk-...
#   ANTHROPIC_API_KEY=sk-ant-...
#   GOOGLE_API_KEY=AI...
```

</details>

<details>
<summary><b>Option B: Local LLMs only</b> â€” no API keys needed</summary>

```bash
# 1. Install Ollama â†’ https://ollama.com/download
# 2. Pull any models
ollama pull llama3.2
ollama pull qwen3:4b
ollama pull mistral

# 3. Ensure Ollama is running
ollama serve
```

D-MMCE auto-discovers all installed Ollama models â€” no configuration needed.

</details>

<details>
<summary><b>Option C: Mixed</b> â€” cloud + local together</summary>

Set API keys in `.env` AND have Ollama running. D-MMCE will use everything available.

</details>

> **Tip:** You need *at least one* working provider. The engine auto-detects what's available and gracefully skips the rest.

### 3. Run

**Web UI (recommended):**

```bash
python server.py
# Open http://localhost:8000
```

**CLI:**

```bash
python main.py "What causes the Northern Lights?"
```

---

## ğŸŒ Web UI

The web interface provides a full real-time view of the D-MMCE pipeline:

| Feature | Description |
|---|---|
| ğŸ² **Pipeline Stages** | Animated chip indicators for each stage: Diversify â†’ Infer â†’ Peer Review â†’ Cluster â†’ Synthesize â†’ Verdict |
| ğŸ“¡ **Live Event Feed** | Streaming log of every pipeline event as it happens |
| âœï¸ **Token Streaming** | Live typing effect â€” tokens appear in response cards as they're generated by Ollama models |
| ğŸ§  **Model Response Cards** | Individual outputs from each model Ã— prompt variant, rendered live as they arrive |
| ğŸ† **Globally Optimal Answer** | The final synthesised result with stability score |
| ğŸ“Š **Stability Gauge** | Circular SVG gauge showing convergence quality (green â‰¥ 85%, yellow â‰¥ 60%, red < 60%) |
| ğŸ“‹ **Audit Trail** | Full step-by-step breakdown of every pipeline decision |
| ğŸ“œ **Run History** | Browse, view details, and delete past runs â€” all stored in local SQLite |
| âš™ï¸ **Settings Panel** | Configure API keys, toggle cloud providers, select local Ollama models, tune stability threshold, max re-runs, concurrency, retries, and streaming |
| ğŸ¦™ **Local LLM Picker** | Auto-discovers all Ollama models with parameter size, quantization, and family info |

---

## ğŸ¦™ Using Local LLMs (Ollama)

D-MMCE works with **any model** installed on your local [Ollama](https://ollama.com) instance. Multiple local models participate simultaneously, each as a distinct ensemble member (`ollama:qwen3:4b`, `ollama:llama3.2:3b`, etc.).

### Setup

```bash
# Install Ollama â†’ https://ollama.com/download
# Pull models
ollama pull llama3.2:3b
ollama pull qwen3:4b
ollama pull mistral
ollama pull phi3
ollama pull codellama

# Start the server
ollama serve
```

### CLI Usage

```bash
# Auto-discover local models (no flags needed â€” D-MMCE finds them)
python main.py "Explain transformers"

# Explicitly add specific local models
python main.py --ollama-model mistral --ollama-model phi3 "Compare sorting algorithms"

# Mix cloud + local
python main.py --providers openai --ollama-model mistral "What is dark matter?"
```

### Web UI Usage

1. Open the Settings panel (âš™ï¸ button)
2. Scroll to **ğŸ¦™ Local LLMs** â€” the UI auto-discovers all installed Ollama models
3. Click to select/deselect any model
4. Save & run your query â€” selected models join the ensemble

### Programmatic Usage

```python
import asyncio
from d_mmce.providers.ollama_provider import OllamaProvider

async def main():
    # List all available local models
    models = await OllamaProvider.list_local_models()
    for m in models:
        print(f"{m['name']:20s}  {m['parameter_size']:>5s}  {m['quantization']}")

asyncio.run(main())
```

---

## ğŸ“– CLI Reference

| Flag | Default | Description |
|---|---|---|
| `query` | *(required)* | The question or task to process |
| `--providers` | auto-discover all | Comma-separated list (e.g. `openai,anthropic`, `ollama:mistral`) |
| `--ollama-model` | â€” | Add a local Ollama model; repeatable for multiple models |
| `--review-provider` | `auto` | Model for peer reviews (`auto` = best available provider) |
| `--embedding-model` | `all-MiniLM-L6-v2` | sentence-transformers model for semantic clustering |
| `--stability-threshold` | `0.85` | Cosine similarity required for Stability Loop convergence |
| `--max-reruns` | `3` | Maximum synthesis re-runs in the Stability Loop |
| `--max-concurrent` | `4` | Maximum parallel inference tasks (semaphore limit) |
| `--max-retries` | `2` | Retries per task with exponential backoff |
| `--no-streaming` | off | Disable token streaming (use batch mode) |
| `-v, --verbose` | off | Enable DEBUG logging |

---

## ğŸ Programmatic API

```python
import asyncio
from d_mmce import D_MMCE

async def main():
    engine = D_MMCE(
        stability_threshold=0.85,
        max_stability_reruns=3,
        review_provider_name="auto",    # picks best available
        max_concurrent_tasks=4,         # limit parallel calls
        max_retries=2,                  # retry with exponential backoff
        enable_streaming=True,          # token-by-token streaming
    )

    verdict = await engine.run("What is dark matter?")

    print(verdict.answer)
    print(f"Stability: {verdict.stability_score:.2%}")
    print(f"Re-runs:   {verdict.num_reruns}")
    for step in verdict.audit_trail:
        print(f"  â€¢ {step}")

asyncio.run(main())
```

### Subscribe to Pipeline Events

```python
from d_mmce.observer import EventType

def on_response(event):
    data = event.payload
    print(f"[{data['provider']}] {data['variant']}: {data['text'][:100]}...")

engine = D_MMCE()
engine.event_bus.subscribe(EventType.MODEL_RESPONSE, on_response)
engine.event_bus.subscribe(EventType.STABILITY_CHECK, lambda e: print(e.message))
```

### Available Event Types

| Event | Emitted whenâ€¦ |
|---|---|
| `PROMPT_PERTURBED` | Prompt variants are generated |
| `MODEL_RESPONSE` | A model returns a complete response (includes full text in payload) |
| `TOKEN_CHUNK` | A single token is streamed from a provider (includes `provider`, `variant`, `token` in payload) |
| `PEER_CRITIQUE` | A peer review critique is completed |
| `CLUSTER_FORMED` | Semantic clustering identifies the consensus cluster |
| `OUTLIER_DISCARDED` | A response is classified as an outlier |
| `SYNTHESIS_STARTED` | Meta-Judge begins synthesis |
| `STABILITY_CHECK` | A stability loop iteration completes |
| `STABILITY_RERUN` | Synthesis is re-run due to instability |
| `FINAL_VERDICT` | Final answer is produced (includes answer in payload) |

---

## ğŸ—ï¸ Architecture

```
D_MMCE/
â”œâ”€â”€ main.py                          # CLI entry point
â”œâ”€â”€ server.py                        # FastAPI + SSE streaming server
â”œâ”€â”€ requirements.txt                 # All dependencies
â”œâ”€â”€ pytest.ini                       # Test configuration
â”œâ”€â”€ .env.example                     # API key template
â”œâ”€â”€ .gitignore                       # Standard Python gitignore
â”œâ”€â”€ d_mmce.log                       # Runtime debug log (auto-created)
â”œâ”€â”€ d_mmce_history.db                # Run history database (auto-created)
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html                   # Single-page web UI (zero build step)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                  # Shared fixtures & MockProvider
â”‚   â”œâ”€â”€ test_unit.py                 # Unit tests (schemas, observer, perturbator, factory)
â”‚   â”œâ”€â”€ test_integration.py          # Integration & E2E tests (full pipeline)
â”‚   â””â”€â”€ test_features.py             # Streaming, concurrency/retry, history tests
â””â”€â”€ d_mmce/
    â”œâ”€â”€ __init__.py                  # Public API: D_MMCE, FinalVerdict
    â”œâ”€â”€ schemas.py                   # Typed dataclasses (ModelResponse, etc.)
    â”œâ”€â”€ observer.py                  # Observer pattern: EventBus + EventType
    â”œâ”€â”€ prompt_perturbator.py        # Diversity injection (4 prompt variants)
    â”œâ”€â”€ peer_reviewer.py             # Cross-examination / Contradiction Matrix
    â”œâ”€â”€ semantic_clusterer.py        # Embeddings + HDBSCAN consensus clustering
    â”œâ”€â”€ meta_judge.py                # Meta-Judge + Stability Loop
    â”œâ”€â”€ orchestrator.py              # D_MMCE class â€” full async pipeline
    â”œâ”€â”€ history.py                   # SQLite-backed run history (aiosqlite)
    â””â”€â”€ providers/
        â”œâ”€â”€ __init__.py              # Auto-registers all providers
        â”œâ”€â”€ base.py                  # Strategy interface (ModelProvider ABC)
        â”œâ”€â”€ factory.py               # Factory + @register + auto-discovery
        â”œâ”€â”€ openai_provider.py       # OpenAI GPT-4o
        â”œâ”€â”€ anthropic_provider.py    # Anthropic Claude 3.5 Sonnet
        â”œâ”€â”€ gemini_provider.py       # Google Gemini 1.5 Pro
        â””â”€â”€ ollama_provider.py       # Any local model via Ollama HTTP API
```

### Design Patterns

| Pattern | Where | Why |
|---|---|---|
| **Strategy** | `ModelProvider` ABC | Each LLM backend is a swappable strategy; the orchestrator is provider-agnostic |
| **Factory** | `ProviderFactory` + `@register` | Providers self-register on import â€” no central switch statement |
| **Observer** | `EventBus` + `EventType` | Pipeline stages emit typed events for UIs, loggers, and dashboards without coupling |
| **Async Orchestrator** | `D_MMCE.run()` | `asyncio.as_completed()` for parallel inference with live event streaming |

---

## ğŸ›¡ï¸ How the Pipeline Prevents Hallucinations

| Layer | Mechanism |
|---|---|
| **1. Diversity Injection** | Multiple prompt framings prevent a single framing from biasing all models toward the same error |
| **2. Heterogeneous Models** | Different training data, architectures, and alignment approaches make correlated failures unlikely |
| **3. Peer Review** | The "logic auditor" prompt explicitly asks for *failure points and factual inaccuracies*. Hallucinating models get penalised in the Contradiction Matrix |
| **4. Semantic Clustering** | Outlier responses (hallucinations no other model agrees with) are mathematically identified and discarded |
| **5. Stability Loop** | Even after synthesis, if re-generation produces a different answer, the system recognises it hasn't converged and retries |

---

## ğŸ› ï¸ Technical Stack

| Layer | Technology |
|---|---|
| Language | Python 3.10+ |
| Async orchestration | `asyncio.as_completed()` with semaphore-gated concurrency and exponential-backoff retry |
| Token streaming | Ollama NDJSON streaming via `httpx.AsyncClient.stream()` with per-token `TOKEN_CHUNK` events |
| LLM SDKs | `openai`, `anthropic`, `google-generativeai`, `httpx` (Ollama) |
| Embeddings | `sentence-transformers` (`all-MiniLM-L6-v2`) |
| Clustering | `hdbscan` with cosine-similarity fallback |
| Web server | FastAPI + SSE (Server-Sent Events) streaming |
| Frontend | Vanilla HTML/CSS/JS â€” zero build step, dark-themed UI |
| Run history | `aiosqlite` â†’ `d_mmce_history.db` (auto-created, env-configurable via `D_MMCE_HISTORY_DB`) |
| Logging | Python `logging` â†’ console + `d_mmce.log` file |
| Testing | `pytest` + `pytest-asyncio` â€” 68 tests, ~7s, no network calls |

---

## ğŸ§ª Testing

The test suite uses **mock providers** â€” no API keys or Ollama required. Tests run in ~7 seconds.

### Run all tests

```bash
pytest                    # run everything
pytest -v                 # verbose output
pytest tests/test_unit.py # unit tests only
pytest tests/test_integration.py  # integration & E2E only
pytest tests/test_features.py     # streaming, concurrency, history tests
```

### Test structure

| File | Tests | What it covers |
|---|---|---|
| `tests/conftest.py` | â€” | `MockProvider`, `MockStreamingProvider`, `FlakyProvider`, shared fixtures |
| `tests/test_unit.py` | 29 | Schemas, `EventBus` pub/sub, `PromptPerturbator`, `ModelProvider.generate()`, peer review parsing & penalty calculation, `ProviderFactory` registry |
| `tests/test_integration.py` | 20 | `PeerReviewer` full flow, `SemanticClusterer` consensus & outlier detection, `MetaJudge` synthesis & stability convergence, **Full E2E pipeline** |
| `tests/test_features.py` | 19 | **Token Streaming** (supports_streaming, generate_stream, TOKEN_CHUNK events), **Concurrency** (semaphore limits, retry with backoff, exhausted retries), **History DB** (save, list, get, delete, clear, pagination, ordering), **E2E history integration** |

### Key E2E tests

| Test | Validates |
|---|---|
| `test_full_pipeline` | Complete pipeline returns a `FinalVerdict` with a non-empty answer |
| `test_pipeline_emits_all_event_types` | All expected event types fire during a run |
| `test_pipeline_model_response_has_payload` | `MODEL_RESPONSE` events carry `text`, `provider`, `variant` |
| `test_pipeline_final_verdict_has_answer_payload` | `FINAL_VERDICT` event includes the full answer |
| `test_pipeline_deterministic_converges` | Stability Loop converges (score â‰¥ 0.85) with deterministic inputs |
| `test_pipeline_no_providers_returns_error` | Graceful error when no providers are available |

---

## ğŸ”§ Troubleshooting

### Debug Log

Every pipeline run is logged to **`d_mmce.log`** in the project root. Check it for:
- Which providers were discovered and their availability status
- Which model the Meta-Judge selected
- Response lengths and latencies
- Errors, timeouts, and stack traces

```bash
# View the last 50 lines
tail -50 d_mmce.log                     # macOS/Linux
Get-Content d_mmce.log -Tail 50         # Windows PowerShell
```

### Common Issues

| Issue | Solution |
|---|---|
| `No providers available` | Set API keys in `.env` or start Ollama with at least one pulled model |
| `ReadTimeout` on large models | Reduce **Max Concurrent Tasks** in settings (default 4). Large models (20B+) benefit from concurrency of 1â€“2. The retry system will auto-recover from transient timeouts |
| `404 Not Found` from Ollama | The model tag doesn't match what's installed. Run `ollama list` and use the exact tag (e.g. `qwen3:4b` not `qwen3`) |
| Response cards not showing | Open browser DevTools (F12) â†’ Console. Look for `[D-MMCE]` logs. Hard-refresh with `Ctrl+Shift+R` |
| Token streaming not working | Ensure **Enable Token Streaming** is checked in settings. Only Ollama providers support true streaming; cloud providers use batch mode |
| History not loading | The SQLite database is created automatically at `d_mmce_history.db`. Set `D_MMCE_HISTORY_DB` env var to change the location |
| Old code still running | Kill stale Python processes and restart: `Get-Process python* \| Stop-Process -Force` (Windows) or `pkill -f server.py` (Linux/macOS), then `python server.py` |

### Browser Console Debugging

The web UI logs all pipeline events to the browser console with the `[D-MMCE]` prefix:
```
[D-MMCE] MODEL_RESPONSE { provider: "ollama:qwen3:4b", hasText: true }
[D-MMCE] FINAL_VERDICT  { hasPayload: true, answer_length: 1275 }
```

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE) for details.

---

<div align="center">

**Built to prove that the wisdom of the crowd applies to machines too.**

<sub>D-MMCE â€¢ Dynamic Multi-Model Consensus Engine</sub>

</div>
